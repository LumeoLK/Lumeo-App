{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dedba936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in .\\venv\\Lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\venv\\Lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\venv\\Lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\venv\\Lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\venv\\Lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in .\\venv\\Lib\\site-packages (from matplotlib) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\venv\\Lib\\site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\venv\\Lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\venv\\Lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\venv\\Lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in .\\venv\\Lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c6025886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm #to view the progress bar\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from matplotlib import pyplot as plt\n",
    "import ast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53dd8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since datasetImg couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\thayalan\\.cache\\huggingface\\datasets\\dataset_img\\default\\0.0.0\\2c0e2d4d5df9bf2d (last modified on Fri Feb  6 17:43:50 2026).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"datasetImg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a03f9789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['image', 'label']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5735c2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10364"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fedaada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Created manifest with 10364 entries.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define your base directory (current folder)\n",
    "base_path = Path.cwd()\n",
    "dataset_dir = base_path / \"furniture_dataset_folder\"\n",
    "\n",
    "data_list = []\n",
    "\n",
    "# Iterate through folders\n",
    "for img_path in dataset_dir.rglob(\"*.*\"):\n",
    "    if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        \n",
    "        # Get the path relative to the project root\n",
    "        relative_path = img_path.relative_to(base_path)\n",
    "        \n",
    "        category = img_path.parent.name\n",
    "        \n",
    "        data_list.append({\n",
    "            \"image_id\": img_path.stem,\n",
    "            \"relative_path\": str(relative_path),\n",
    "            \"category\": category,\n",
    "            \"furniture_colors\": \"\" \n",
    "        })\n",
    "\n",
    "# 3. Create and Save CSV\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv(\"furniture_metadata.csv\", index=False)\n",
    "\n",
    "print(f\"Done! Created manifest with {len(df)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dd0d77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>relative_path</th>\n",
       "      <th>category</th>\n",
       "      <th>furniture_colors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>015fe660-4dc3-41f8-a99c-77af140f5478</td>\n",
       "      <td>furniture_dataset_folder\\Dresser Dataset\\2025-...</td>\n",
       "      <td>2025-07-08T22-19-21.239Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02115e66-d076-4e90-bd6b-45872ad50b34</td>\n",
       "      <td>furniture_dataset_folder\\Dresser Dataset\\2025-...</td>\n",
       "      <td>2025-07-08T22-19-21.239Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>041bb782-b940-4dc9-acac-65e7c1da057b</td>\n",
       "      <td>furniture_dataset_folder\\Dresser Dataset\\2025-...</td>\n",
       "      <td>2025-07-08T22-19-21.239Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>052c37db-9396-4b11-9a0e-351a1fba5a00</td>\n",
       "      <td>furniture_dataset_folder\\Dresser Dataset\\2025-...</td>\n",
       "      <td>2025-07-08T22-19-21.239Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>066656ee-4af6-4ed4-8ae6-2ce8b4f35ca5</td>\n",
       "      <td>furniture_dataset_folder\\Dresser Dataset\\2025-...</td>\n",
       "      <td>2025-07-08T22-19-21.239Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               image_id  \\\n",
       "0  015fe660-4dc3-41f8-a99c-77af140f5478   \n",
       "1  02115e66-d076-4e90-bd6b-45872ad50b34   \n",
       "2  041bb782-b940-4dc9-acac-65e7c1da057b   \n",
       "3  052c37db-9396-4b11-9a0e-351a1fba5a00   \n",
       "4  066656ee-4af6-4ed4-8ae6-2ce8b4f35ca5   \n",
       "\n",
       "                                       relative_path  \\\n",
       "0  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
       "1  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
       "2  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
       "3  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
       "4  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
       "\n",
       "                   category furniture_colors  \n",
       "0  2025-07-08T22-19-21.239Z                   \n",
       "1  2025-07-08T22-19-21.239Z                   \n",
       "2  2025-07-08T22-19-21.239Z                   \n",
       "3  2025-07-08T22-19-21.239Z                   \n",
       "4  2025-07-08T22-19-21.239Z                   "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff4889",
   "metadata": {},
   "source": [
    "Detecting the room type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b91ac1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model (load only once)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(f\"CLIP loaded on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f47457eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Room label\n",
    "room_labels = [\n",
    "    \"living room\",\n",
    "    \"bedroom\",\n",
    "    \"kitchen\",\n",
    "    \"bathroom\",\n",
    "    \"dining room\",\n",
    "    \"home office\",\n",
    "    \"hallway\",\n",
    "    \"study room\",\n",
    "    \"balcony\",\n",
    "    \"garage\",\n",
    "    \"gaming room\"\n",
    "]\n",
    "\n",
    "# Precompute text features (embeddings don't change. Run only once)\n",
    "text_tokens = clip.tokenize(room_labels).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f801b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_room_type(image_path, top_k=1):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\") #opens the image using Pillow\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "    img_input = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_features = model.encode_image(img_input) #encodes the images\n",
    "        img_features /= img_features.norm(dim=-1, keepdim=True) #normalize\n",
    "\n",
    "        # Cosine similarity → softmax for probs\n",
    "        logits = (100.0 * img_features @ text_features.T).softmax(dim=-1)\n",
    "        probs = logits.cpu().numpy()[0]\n",
    "\n",
    "    # Top results\n",
    "    top_idx = np.argsort(probs)[::-1][:top_k]\n",
    "    \n",
    "    print(\"Top match:\")\n",
    "    results = []\n",
    "    for i, idx in enumerate(top_idx):\n",
    "        score = probs[idx] * 100\n",
    "        label = room_labels[idx]\n",
    "        print(f\"{label}\")\n",
    "        results.append((score, label))\n",
    "\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94120bb2",
   "metadata": {},
   "source": [
    "Extracting the colours of the img in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbc3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rembg import remove\n",
    "import io\n",
    "\n",
    "def extract_furniture_color_only(image_path, n_colors=3):\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        return \"File Not Found\"\n",
    "\n",
    "    #Load and Remove Background\n",
    "    with open(image_path, 'rb') as i:\n",
    "        input_image = i.read()\n",
    "    \n",
    "    # This returns an RGBA image (Alpha channel is 0 for background)\n",
    "    output_rgba = remove(input_image)\n",
    "    img = Image.open(io.BytesIO(output_rgba)).convert(\"RGBA\")\n",
    "    \n",
    "    # Prepare pixels for K-Means\n",
    "    # Using pixels where the Alpha channel (mask) is not 0\n",
    "    np_img = np.array(img)\n",
    "    rgb_pixels = np_img[:, :, :3]\n",
    "    alpha_mask = np_img[:, :, 3]\n",
    "    \n",
    "    # Filter out background pixels\n",
    "    furniture_pixels = rgb_pixels[alpha_mask > 0]\n",
    "    \n",
    "    if len(furniture_pixels) == 0:\n",
    "        return \"No object detected\"\n",
    "\n",
    "    # K-Means on isolated pixels\n",
    "    # Use a smaller sample if the image is huge to maintain speed\n",
    "    if len(furniture_pixels) > 10000:\n",
    "        idx = np.random.choice(len(furniture_pixels), 10000, replace=False)\n",
    "        furniture_pixels = furniture_pixels[idx]\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_colors, n_init=10, random_state=42)\n",
    "    kmeans.fit(furniture_pixels)\n",
    "    \n",
    "    # Get dominant colors (RGB)\n",
    "    colors = kmeans.cluster_centers_.astype(int)\n",
    "    \n",
    "    # Sort by frequency\n",
    "    labels, counts = np.unique(kmeans.labels_, return_counts=True)\n",
    "    sorted_colors = colors[np.argsort(-counts)]\n",
    "    \n",
    "    return [tuple(c) for c in sorted_colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_colour_extraction(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Storage for the new data\n",
    "    furniture_palettes = []\n",
    "    \n",
    "    print(\"Starting background-aware color extraction...\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        path = row['relative_path']\n",
    "        try:\n",
    "            colors = extract_furniture_color_only(path, n_colors=3)\n",
    "            furniture_palettes.append(str(colors))\n",
    "        except Exception as e:\n",
    "            furniture_palettes.append(f\"Error: {str(e)}\")\n",
    "            \n",
    "    df['furniture_colors'] = furniture_palettes\n",
    "    df.to_csv(\"furniture_metadata_final.csv\", index=False)\n",
    "    print(\"Done! Metadata updated with isolated furniture colors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592840a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting background-aware color extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10364/10364 [6:26:27<00:00,  2.24s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Metadata updated with isolated furniture colors.\n",
      "                               image_id  \\\n",
      "0  015fe660-4dc3-41f8-a99c-77af140f5478   \n",
      "1  02115e66-d076-4e90-bd6b-45872ad50b34   \n",
      "2  041bb782-b940-4dc9-acac-65e7c1da057b   \n",
      "3  052c37db-9396-4b11-9a0e-351a1fba5a00   \n",
      "4  066656ee-4af6-4ed4-8ae6-2ce8b4f35ca5   \n",
      "\n",
      "                                       relative_path  \\\n",
      "0  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
      "1  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
      "2  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
      "3  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
      "4  furniture_dataset_folder\\Dresser Dataset\\2025-...   \n",
      "\n",
      "                   category                                   furniture_colors  \n",
      "0  2025-07-08T22-19-21.239Z  [(np.int64(214), np.int64(211), np.int64(206))...  \n",
      "1  2025-07-08T22-19-21.239Z  [(np.int64(63), np.int64(59), np.int64(57)), (...  \n",
      "2  2025-07-08T22-19-21.239Z  [(np.int64(2), np.int64(171), np.int64(213)), ...  \n",
      "3  2025-07-08T22-19-21.239Z  [(np.int64(171), np.int64(109), np.int64(77)),...  \n",
      "4  2025-07-08T22-19-21.239Z  [(np.int64(122), np.int64(71), np.int64(45)), ...  \n"
     ]
    }
   ],
   "source": [
    "# img_colour_extraction(\"furniture_metadata.csv\")\n",
    "\n",
    "# new_df = pd.read_csv(\"furniture_metadata_final.csv\")\n",
    "# print(new_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5edf9",
   "metadata": {},
   "source": [
    "Colour platte extraction of the user image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "745b84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_color_palette(image_path, n_colors=3, resize_to=(400, 400)):\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, resize_to)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    pixels = img_rgb.reshape(-1, 3).astype(np.float32)\n",
    "    kmeans = KMeans(n_clusters=n_colors, n_init=10, random_state=42)\n",
    "    kmeans.fit(pixels)\n",
    "    colors = kmeans.cluster_centers_.astype(int)\n",
    "    return [tuple(c) for c in colors]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the distance of the colour palette\n",
    "def calculate_color_distance(palette1, palette2):\n",
    "   \n",
    "    p1 = np.array(palette1)\n",
    "    p2 = np.array(palette2)\n",
    "    \n",
    "    # Calculate Euclidean distance between colors\n",
    "    distances = np.linalg.norm(p1 - p2, axis=1)\n",
    "    \n",
    "    return np.mean(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "027f1dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_list(x):\n",
    "    if pd.isna(x) or not isinstance(x, str):\n",
    "        return []\n",
    "    # If the string starts with 'Error' or 'No object', return empty list\n",
    "    if x.startswith(\"Error\") or x.startswith(\"No\"):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3cc0e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the data\n",
    "def get_recommendations(user_img_path, vector_csv_path, top_k=5):\n",
    "    # Load the database\n",
    "    df = pd.read_csv(vector_csv_path)\n",
    "    \n",
    "    # Handle vectors\n",
    "    df['feature_vector'] = df['feature_vector'].apply(safe_parse_list)\n",
    "    # Filter out any rows that failed to parse into a valid vector\n",
    "    df = df[df['feature_vector'].map(len) > 0]\n",
    "    df['feature_vector'] = df['feature_vector'].apply(np.array)\n",
    "    \n",
    "    # Handle colors\n",
    "    df['furniture_colors'] = df['furniture_colors'].apply(safe_parse_list)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Error: The database is empty after filtering invalid rows.\")\n",
    "        return None\n",
    "\n",
    "    # Process User Image for CLIP Vector\n",
    "    user_img = Image.open(user_img_path).convert(\"RGB\")\n",
    "    user_input = preprocess(user_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        user_vec = model.encode_image(user_input)\n",
    "        user_vec /= user_vec.norm(dim=-1, keepdim=True)\n",
    "        user_vec = user_vec.cpu().numpy()\n",
    "    \n",
    "    # Process User Image for Colors\n",
    "    user_palette = extract_color_palette(user_img_path)\n",
    "\n",
    "    #Calculate Visual Similarity (CLIP)\n",
    "    dataset_vecs = np.stack(df['feature_vector'].values)\n",
    "    visual_sims = cosine_similarity(user_vec, dataset_vecs).flatten()\n",
    "    \n",
    "    # Calculate Colour Similarity\n",
    "    color_scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Check if row has valid colors \n",
    "        if not row['furniture_colors'] or not isinstance(row['furniture_colors'], list) or len(row['furniture_colors']) == 0:\n",
    "            color_scores.append(1000) \n",
    "        else:\n",
    "            dist = calculate_color_distance(user_palette, row['furniture_colors'])\n",
    "            color_scores.append(dist)\n",
    "    \n",
    "    # Normalize colour score \n",
    "    max_dist = max(color_scores) if max(color_scores) > 0 else 1\n",
    "    norm_color_score = 1 - (np.array(color_scores) / max_dist)\n",
    "    \n",
    "    #Final Weighted Score\n",
    "    df['final_score'] = (visual_sims * 0.7) + (norm_color_score * 0.3)\n",
    "    \n",
    "    return df.sort_values(by='final_score', ascending=False).head(top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64ccf3",
   "metadata": {},
   "source": [
    "Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcedc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top match:\n",
      "gaming room\n",
      "Palette saved to: palette_output.jpg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image = os.path.join(\"images\", \"images.jpg\")   \n",
    "    \n",
    "    if not os.path.exists(image):\n",
    "        print(\"Image not found!\")\n",
    "    else:\n",
    "        try:\n",
    "            infer_room_type(image, top_k=1)\n",
    "            palette = extract_color_palette(image, n_colors=6)\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
